\documentclass{article}
\input{preamble.tex}
\begin{document}
\section*{Wilks Theorem for Dynamical ARX systems under equality constraints}
\subsection*{Notation}
We have the model of order $n$:
\begin{equation}
    \begin {split}
        \mathcal{P}:\, y[k] &= \varphi^T[k]\theta_\star + w[k]; \;
        w[k]\sim \mathcal{N}(0, \gamma^2)\\
                \varphi[k] &= \begin{bmatrix}
                        -y_{[k-1:k-n]} & u_{[k-1:k-n]}  
                \end{bmatrix}^T \\
                y_{[k-1:k-n]} &= \begin{bmatrix}
                        y[k-1] & y[k-2] & \cdots & y[k-n]
                \end{bmatrix}^T \\
                u_{[k-1:k-n]}  &= \begin{bmatrix}
                        u[k-1] & u[k-2] & \cdots & u[k-n]
                \end{bmatrix}^T
        \end{split}
        \label{eq:model}
\end{equation}
where an estimate $\hat{\theta}$ of $\theta_\star$ is computed from data using least squares with the loss function
\begin{equation}
    \ell(\hat{\theta}) = (Y-\Phi^T\hat{\theta})^T(Y-\Phi^T\hat{\theta})
    \label{eq:loss}
\end{equation}
where the data matrices $Y$ and $\Phi$ are structured as such:
\begin{equation}
    Y = \begin{bmatrix}
        y[1] \\ y[2] \\ \vdots \\ y[N]
    \end{bmatrix}, \;\; \Phi = \begin{bmatrix}
        \varphi[1] & \varphi[2] & \cdots && \varphi[N]
    \end{bmatrix}.
\end{equation}
We want to investigate the asymptotic distribution of the log-likelihood
ratio
\begin{equation}
    R_L \triangleq \left(
    \sup\limits_{\tilde{\theta} \in \Mp}
    \ell(\tilde\theta \big{\vert} Y, \Phi)-
    \sup\limits_{\hat\theta \in \mathbb{R}^{2n}}
    \ell(\hat\theta \big{\vert} Y, \Phi) 
    \right)
\end{equation}
which compares the hypothesis 
\begin{equation}
    \mathcal{H}_1: \theta_\star \in \Mp = \left\{ \theta
    \vert
    A\theta=\omega\right\}, \; A^TA = I
\end{equation}
with the null
\begin{equation}
    \mathcal{H}_0: \theta_\star \in \mathbb{R}^{2n}.
\end{equation}
We have from the equality constraint defining $\Mp$
\begin{equation}
    \centering
        \begin{split}
            \theta &= A^+\omega + \nullA\nu \\
            \text{where}& \\
            \omega \in \mathbb{R}^{n_e},& \; \nu \in \mathbb{R}^{n_c}, \; n_e+n_c=2n \\
            A^+ &= A^T(AA^T)^{-1} \in \mathbb{R}^{2n\times n_e}\\
            \nullA & \in \mathbb{R}^{2n\times n_c} \text{ whose columns form a basis for} \ker A\\
        \end{split}
    \label{eq:affinemap}
\end{equation}
$\omega$ thus represents prior knowledge of the model, where some linear combinations of $\theta_\star$ are
known beforehand. $\nu$ are the remaining variables in $\nullA$-space that still need to be estimated.

$\hat{\theta} = A^+\hat\omega + K_A \hat{\nu}$ refers to the unconstrained estimate of $\theta_\star$, and
$\tilde{\theta} = A^+\omega + K_A \tilde{\nu}$ refers to the constrained
estimate where $\omega$ is taken as prior knowledge.

For a general $\theta$, we have the following lemma.
\begin{lemma}
    Given the affine map (\ref{eq:affinemap}) that maps $\theta$ to $\omega, \nu$,
    the inverse map from $\omega, \nu$ to $\theta$ is given by:
    \begin{equation*}
        \begin{bmatrix}
            \omega \\ \nu
        \end{bmatrix} = \begin{bmatrix}
            A^+ & K_A
        \end{bmatrix}^T\theta.
    \end{equation*}
\label{lem:inv_map}
\end{lemma}
\begin{proof}
    Rewrite (\ref{eq:affinemap}) as 
    \[
    \theta = \begin{bmatrix}
        A^+ & \nullA
    \end{bmatrix}
    \begin{bmatrix}
        \omega \\ \nu
    \end{bmatrix}.
    \]
    Both $A^+$ and $\nullA$ are semiorthogonal, with their columns being basis vectors for $\text{im } A$ and
    $\ker A$ respectively. Therefore, the matrix $\begin{bmatrix}
        A^+ & \nullA
    \end{bmatrix}$ is orthogonal and has a range space of $\text{im } A \oplus \ker A = \mathbb{R}^{2n}$, with
    it's inverse being it's own transpose. To get lemma \ref{lem:inv_map}, left-multiply the above equation with this transpose.
\end{proof}


\subsection*{Distribution of the MLE estimator}

First, some prerequisite results from \cite{soderstrom_system_nodate}.
Given the above model, and a Maximum Likelihood estimate
$\hat{\theta} \in \mathbb{R}^{2n}$ we have the asymptotic distribution of the error
as the number of samples $N \rightarrow  \infty$, given that the data
$\{\Phi, Y\}$ is produced by the model (\ref{eq:model}):
\begin{equation}
        \sqrt{N}(\hat{\theta}-\theta_\star)
        \xrightarrow[N\rightarrow\infty]{\text{dist}}
        \mathcal{N}(0, \gamma^2(\mathbb{E}\varphi\varphi^T)^{-1})
\end{equation}

Let $\Sigma_\varphi = \gamma^2(\mathbb{E}\varphi\varphi^T)^{-1}$ be the covariance of the MLE estimator.
The distribution of the estimation error can be expressed in terms of $\omega, \nu$ in
accordance with the following lemma:
\begin{lemma}
    The estimation error in terms of $\omega$ and $\nu$ is obtained by the linear transformation
    \[
    \begin{bmatrix}
            A^+ & K_A
    \end{bmatrix}^T
    \sqrt{N}(\hat{\theta}-\theta_\star) =
    \sqrt{N}\begin{bmatrix}
        \hat{\omega}-\omega_\star \\ \hat{\nu} - \nu_\star
    \end{bmatrix}
    \sim \mathcal{N}(0, \Sigma_{\mathcal{M}})
    \]
    and has the covariance matrix
    \begin{equation}
        \begin{gathered}
        \Sigma_{\mathcal{M}} = 
        \begin{bmatrix}
        \Sigma_{\omega\omega} & \Sigma_{\omega\nu} \\
        \Sigma_{\nu\omega} & \Sigma_{\nu\nu}
        \end{bmatrix} \\
        = \begin{bmatrix}
        (A^+)^T\Sigma_\varphi A^+ & (A^+)^T\Sigma_\varphi \nullA \\
        \nullA^T\Sigma_\varphi A^+ & K_A^T\Sigma_\varphi \nullA
        \end{bmatrix}
        \end{gathered}
    \end{equation}

    Furthermore, the inverse of its covariance matrix is given by
    \begin{equation}
        \begin{gathered}
            \Sigma_{\mathcal{M}}^{-1}
            =\begin{bmatrix}
                (A^+)^T\Sigma_\varphi^{-1} A^+ & (A^+)^T\Sigma_\varphi^{-1} K_A \\[5pt]
                K_A^T\Sigma_\varphi^{-1} A^+ & K_A^T\Sigma_\varphi^{-1} K_A
            \end{bmatrix}
        \end{gathered}
        \label{eq:affinecovariance}
    \end{equation}
    \label{lem:omeganudistribution}
\end{lemma}

\begin{proof}
    The distribution of $\sqrt{N}\begin{bmatrix}
        \hat{\omega}-\omega_\star \\ \hat{\nu} - \nu_\star
    \end{bmatrix}$ follows immediately from lemma \ref{lem:inv_map}, where the covariance
    matrix $\Sigma_{\mathcal{M}}$ is given by
    $\begin{bmatrix}
        A^+ & \nullA
        \end{bmatrix}^T\Sigma_\varphi\begin{bmatrix}
        A^+ & \nullA
    \end{bmatrix}$.
    Since 
    $\begin{bmatrix}
        A^+ & K_A
    \end{bmatrix}^T$
    is an orthogonal matrix, the inverse covariance matrix can be computed as such
    \[
    \Sigma_{\mathcal{M}}^{-1} =
    \left(\begin{bmatrix}
        A^+ & \nullA
        \end{bmatrix}^T\Sigma_\varphi\begin{bmatrix}
        A^+ & \nullA
    \end{bmatrix}\right)^{-1} =
    \begin{bmatrix}
        A^+ & \nullA
        \end{bmatrix}^T\Sigma_\varphi^{-1}\begin{bmatrix}
        A^+ & \nullA
    \end{bmatrix}
    \]
\end{proof}


\subsection*{Taylor expansions of the loss function}
Turning back to the loss function (\ref{eq:loss}), by expanding
we have
$$
\ell(\theta) = \left(
    Y^TY -2\theta^T\Phi Y + \theta^T\Phi\Phi^T\theta.
    \right)
$$
Replacing $\theta$ with the affine map (\ref{eq:affinemap}), we can equivalently write
the loss function as a function of $\omega$ and $\nu$ instead.
\begin{equation}
\begin{gathered}
        \ell(\omega, \nu) = \left(Y^TY - 2(A^+\omega)^T\Phi Y - 2\nu^TK_A^T\Phi Y + 2(A^+\omega)^T\Phi\Phi^TK_A\nu + \right.\\
        \left.+ (A^+\omega)^T\Phi\Phi^TA^+\omega + \nu^TK_A^T\Phi\Phi^TK_A\nu \right)
\end{gathered}
\label{eq:constrainedloss}
\end{equation}

Then, we can equivalently write $R_L$ according to the following lemma.
\begin{lemma}
    Let 
    \begin{equation}
        \hess_\ell = \begin{bmatrix}
                \hess_{\omega\omega} & \hess_{\omega\nu} \\
                \hess_{\nu\omega} & \hess_{\nu\nu}
            \end{bmatrix}
            \label{eq:hessian}
    \end{equation}
    be the Hessian of the loss function $\ell(\omega, \nu)$.
    $R_L =  \left(\ell(\omega, \tilde\nu)-\ell(\hat\omega, \hat\nu)\right)$
    can equivalently be expressed in terms of just
    $\omega-\hat{\omega}$ and the Schur complement of $\hess_{\omega\omega}$ in its Hessian as so:
    \begin{equation}
    \begin{gathered}
        \ell(\omega, \tilde\nu)-\ell(\hat\omega, \hat\nu) = \\ 
        = (\omega-\hat\omega)^T\left(\hess_{\omega\omega} - \hess_{\omega\nu}
        \hess_{\nu\nu}^{-1}\hess_{\nu\omega}\right)(\omega-\hat\omega)
    \end{gathered}
    \label{eq:shur}
\end{equation}
\end{lemma}
\begin{proof}
We do a Taylor expansion of the constrained estimate $\ell(\omega, \tilde{\nu})$ around the unconstrained estimate
$\omeganu[&]{\hat}{\hat}{^T}{^T}^T$:

\begin{equation*}
    \begin{gathered}
        \ell(\omega, \tilde\nu) = \ell(\hat\omega, \hat\nu) + \nabla\ell(\hat\omega, \hat\nu)\omeganudiff + \frac{1}{2}\omeganudiff^T\hess_\ell\omeganudiff
    \end{gathered}
\end{equation*}
Since the estimator $\omeganu[&]{\hat}{\hat}{^T}{^T}^T$ 
by definition occurs at the point $\nabla\ell(\hat\omega, \hat\nu) = 0$, we have that that term vanishes and we can rewrite
\begin{equation}
    \left(\ell(\omega, \tilde\nu)- \ell(\hat\omega, \hat\nu)\right) = \omeganudiff^T\hess_\ell\omeganudiff
    \label{eq:taylordiff}
\end{equation}
Next, we do the Taylor expansion of $\nabla\ell(\omega, \tilde{\nu})$ around $\nabla\ell(\hat\omega, \hat\nu)$
\begin{equation*}
    \begin{bmatrix}
        \nabla_\omega \\ \nabla_{\tilde{\nu}}
    \end{bmatrix}\ell(\omega, \tilde{\nu}) = \nabla\ell(\hat\omega, \hat\nu) +
    \begin{bmatrix}
            \hess_{\omega\omega} & \hess_{\omega\nu} \\
            \hess_{\nu\omega} & \hess_{\nu\nu}
    \end{bmatrix}\omeganudiff
\end{equation*}
Once again, we use that by definition $\nabla\ell(\hat\omega, \hat\nu) = 0$ and $\nabla_{\tilde\nu}\ell(\omega, \tilde{\nu}) = 0$, and rewrite
$\tilde\nu-\hat\nu$ in terms of $\omega-\hat\omega$.
\begin{equation*}
    \begin{gathered}
        \hess_{\nu\nu}(\tilde{\nu} - \hat{\nu}) = -\hess_{\nu\omega}(\omega-\hat\omega) \\
        (\tilde{\nu} - \hat{\nu}) = -\hess_{\nu\nu}^{-1}\hess_{\nu\omega}(\omega-\hat\omega) \\
        \omeganudiff = \partialhess(\omega-\hat\omega)
    \end{gathered}
\end{equation*}
Using this result, we can rewrite (\ref{eq:taylordiff}) as such
\begin{equation*}
    \begin{gathered}
        \ell(\omega, \tilde\nu)-\ell(\hat\omega, \hat\nu) = \\ 
         = \frac{1}{2}(\omega-\hat\omega)^T \partialhess^T
        \begin{bmatrix}
            \hess_{\omega\omega} & \hess_{\omega\nu} \\
            \hess_{\nu\omega} & \hess_{\nu\nu}
        \end{bmatrix}\partialhess(\omega-\hat\omega) = \\
        = (\omega-\hat\omega)^T\left(\hess_{\omega\omega} - \hess_{\omega\nu}
        \hess_{\nu\nu}^{-1}\hess_{\nu\omega}\right)(\omega-\hat\omega)
    \end{gathered}
\end{equation*}
\end{proof}

\subsection*{The Hessian and the Covariance}
Since the middle factor of (\ref{eq:shur}) is the Schur complement of
$\hess_{\omega\omega}$
in $\hess_\ell$, we have by the block matrix inversion formula that
$
\left(\hess_{\omega\omega} - \hess_{\omega\nu}
        \hess_{\nu\nu}^{-1}\hess_{\nu\omega}\right)^{-1} = \hess_{\ell[1:n_c,1:n_c]}^{-1}
$, the square $n_c\times n_c$ top left block of the inverse of the Hessian.

By differentiating (\ref{eq:constrainedloss}) twice, we get an explicit
formulation of the Hessian (\ref{eq:hessian})
\begin{equation}
    \hess_\ell = 2\begin{bmatrix}
            (A^+)^T\Phi\Phi^TA^+ & (A^+)^T\Phi\Phi^T\nullA \\
            \nullA^T\Phi\Phi^TA^+ & \nullA^T\Phi\Phi^T\nullA
        \end{bmatrix}
\end{equation}
Since $\lim_{N\rightarrow\infty} \Phi\Phi^T/N = \mathbb{E}\varphi\varphi^T$,
we compare with (\ref{eq:affinecovariance}) and see that
\begin{equation}
    \hess_\ell\frac{1}{N} \xrightarrow[N\rightarrow\infty]{} \Sigma_{\mathcal{M}}^{-1}\gamma^2
\end{equation}
and therefore the Schur complement
\begin{equation}
    \frac{1}{N}\left(\hess_{\omega\omega} - \hess_{\omega\nu}
        \hess_{\nu\nu}^{-1}\hess_{\nu\omega}\right) = \Sigma_{\omega\omega}^{-1}\gamma^2
\end{equation}
is proportional by a a factor $1/(\gamma^2N)$ to the inverse of the covariance of the marginal distribution of
$\sqrt{N}(\omega-\hat\omega)$.
Using all of this, we can rewrite (\ref{eq:shur}) as
\begin{equation}
    \begin{gathered}
        \frac{\ell(\omega, \tilde\nu) - \ell(\hat\omega, \hat\nu)}{\gamma^2}
        \xrightarrow[N\rightarrow\infty]{}
        (\omega-\hat\omega)^T\left(\hess_{\omega\omega} - \hess_{\omega\nu}
        \hess_{\nu\nu}^{-1}\hess_{\nu\omega}\right)(\omega-\hat\omega)\frac{N}{N} = \\
        =\sqrt{N}(\omega-\hat\omega)^T \Sigma_{\omega\omega}^{-1} (\omega-\hat\omega)\sqrt{N}
        \sim \mathcal{\chi}^2(n_e)
    \end{gathered}
\end{equation}
Or more concisely that
\begin{equation}
    \frac{R_L}{\gamma^2} \sim \mathcal{\chi}^2(n_e)
\end{equation}

% Using these results, we define the estimation error
% \begin{equation}
%     E_N \triangleq \mathcal{I}^{\frac{1}{2}}(\theta_\star)(\theta_\star-\hat{\theta})
%     \xrightarrow[N\rightarrow\infty]{\text{dist}}
%     \mathcal{N}(0, I_{2n})
% \end{equation}
% giving us
% \begin{equation}
%     (\theta_\star-\hat{\theta}) = \mathcal{I}(\theta_\star)^{-\frac{1}{2}}E_N
%     \label{eq:differr}
% \end{equation}
% Next, we need the log likelihood $\ell(\theta)$ of the MLE and its
% derivatives:
% \begin{equation}
%     \begin{split}
%         \ell(\theta) &= -\frac{1}{2\gamma^2}\sum\limits_{k=0}^N
%         \left[ y^T[k]y[k] - 2\theta^T\varphi[k]y[k] +
%         \theta^T\varphi[k]\varphi^T[k]\theta \right] -
%         \frac{N}{2}\log(2\pi\gamma) \\
%         \ell'(\theta) &= -\frac{1}{\gamma^2}\sum\limits_{k=0}^N\left[
%             \varphi[k]\varphi^T[k]\theta - \varphi[k]y[k]
%         \right] \\
%         \ell''(\theta) &= -\frac{1}{\gamma^2}
%         \sum\limits_{k=0}^N\varphi[k]\varphi^T[k]
%     \end{split}
% \end{equation}
% for the taylor expansion of $\ell(\theta_\star)$ around $\hat{\theta}$.
% Writing out and then using (\ref{eq:differr}) we have
% \begin{equation}
%     \begin{split}
%         \ell(\theta_\star) &= \ell(\hat{\theta}) +
%         \ell'(\hat{\theta})(\theta_\star-\hat{\theta}) +
%         \frac{1}{2}(\theta_\star-\hat{\theta})^T
%         \ell''(\hat{\theta})(\theta_\star-\hat{\theta})\\
%         &= \ell(\hat{\theta}) +
%         \ell'(\hat{\theta})(\theta_\star-\hat{\theta}) +
%         \underbrace{
%             \frac{1}{2}E_N^T\mathcal{I}(\theta_\star)^{-\frac{1}{2}}
%             \ell''(\hat{\theta})
%             \mathcal{I}(\theta_\star)^{-\frac{1}{2}}E_N
%         }\limits_{\kappa}
%     \end{split}
% \end{equation}
% Now, taking the asymptotic value of this taylor series, we have
% for the second order term $\kappa$:
% \begin{equation}
%     \kappa = \frac{1}{2}E_N^T
%     \underbrace{\frac{\gamma}{\sqrt{N}}
%     \left(\mathbb{E}\varphi\varphi^T\right)^{-\frac{1}{2}}}
%     \limits_{\mathcal{I}^{-\frac{1}{2}}(\theta_\star)}
%     %
%     \underbrace{\frac{-N}{\gamma^2}\mathbb{E}\varphi\varphi^T}
%     \limits_{\mathbb{E}\ell''(\theta_\star)}
%     %
%     \underbrace{
%     \left(\mathbb{E}\varphi\varphi^T\right)^{-\frac{1}{2}}
%     \frac{\gamma}{\sqrt{N}}}
%     \limits_{\mathcal{I}^{-\frac{1}{2}}(\theta_\star)}
%     E_N = E_N^TE_N
% \end{equation}
% and the first order term $\ell'(\hat{\theta})(\theta_\star-\hat{\theta})$
% vanishes by definition, since the optimal $\hat{\theta}$ occurs at $\ell'(\theta) = 0$  giving us
% \begin{equation}
%     \begin{split}
%         \ell(\theta_\star) \approx \ell(\hat{\theta}) - \frac{1}{2}E_N^TE_N
%         \\
%         \ell(\hat{\theta}) \approx \ell(\theta_\star) + \frac{1}{2}E_N^TE_N
%     \end{split}
%     \label{eq:taylor1}
% \end{equation}

% Now, we look at the constrained estimate
% $\theta= A^+b + \nullA\theta_C$
% and substitute in $\ell(\theta)$. We denote the constrained
% log likelihood by $\ell_C(\theta)$. Note that terms of $\ell_C(\theta)$ that are constant, linear and quadratic in 
% $\theta$ are black, purple, and red, respectively.
% \begin{equation}
%     \begin{split}
%         \ell_C(\theta) &= -\frac{1}{2\gamma^2}\sum\limits_{k=0}^N
%         \bigg{(} y^T[k]y[k] + (A^+b)^T\varphi[k]\varphi^T[k](A^+b) -
%             2y^T[k]\varphi[k](A^+b) \\
%             &-\textcolor{purple}{2\theta_C^T\nullA^T\varphi[k]y[k]+
%                 2\theta_C^T\nullA^T\varphi[k]\varphi^T[k](A^+b)} \\
%             &+\textcolor{red}{\theta_C^T\nullA^T\varphi[k]\varphi^T[k]\nullA\theta_C}\bigg{)} -
%         \frac{N}{2}\log(2\pi\gamma) \\
%         %%%
%         \ell_C'(\theta) =& -\frac{1}{\gamma^2}
%         \sum\limits_{k=0}^N
%         \textcolor{red}{\nullA^T\varphi[k]\varphi^T[k]\nullA\theta_C}
%         + \textcolor{purple}{\nullA^T\varphi[k]\varphi^T[k](A^+b)-\nullA^T\varphi[k]y[k]}\\
%         %%%
%         \ell_C''(\theta) =& -\frac{1}{\gamma^2}
%         \sum\limits_{k=0}^N\textcolor{red}{\nullA^T\varphi[k]\varphi^T[k]\nullA}
%     \end{split}
% \end{equation}
% Now, defining the constrained estimation error and Fisher Information
% matrix analogously to above, we get
% \begin{equation}
%     \begin{split}
%         \text{Asymptotic distribution of the constrained error:}\\
%         \sqrt{N}(\theta_{\star,C}-\hat{\theta}_C) \xrightarrow[N\rightarrow\infty]{\text{dist}}
%         \mathcal{N}(0, \frac{1}{\gamma^2}\mathbb{E}\nullA^T\varphi\varphi^T\nullA) \\
%         \text{Constrained Fisher Information Matrix }\mathcal{I}(\theta):\\
%         \mathcal{I}(\theta_C) = N \frac{1}{\gamma^2}\mathbb{E}\nullA^T\varphi\varphi^T\nullA
%     \end{split}
%     \label{eq:asymptotic_constrained}
% \end{equation}
% under the hypothesis that $A\theta_\star = b; \theta_\star = A^+b + \nullA\theta_{\star,C}$.
% This allows us to construct the \emph{constrained} estimation error
% \begin{equation}
%     \begin{split}
%         E_{N,C} \triangleq \mathcal{I}^{\frac{1}{2}}(\theta_{\star,C})(\theta_{\star,C}-\hat{\theta}_C)
%     \xrightarrow[N\rightarrow\infty]{\text{dist}}
%     \mathcal{N}(0, I_{2n}) \\
%     (\theta_{\star,C}-\hat{\theta}_C) = \mathcal{I}(\theta_{\star,C})^{-\frac{1}{2}}E_{N,C}
%     \end{split}
% \end{equation}
% Proceeding exactly as in the unconstrained case, we have
% \begin{equation}
%     \begin{split}
%         \ell_C(\theta_{\star,C}) &\approx \ell_C(\hat{\theta}_C) - \frac{1}{2}E_{N,C}^T E_{N,C}
%         \\
%         \ell_C(\hat{\theta}_C) &\approx \ell_C(\theta_{\star,C}) + \frac{1}{2}E_{N,C}^T E_{N,C}
%     \end{split}
%     \label{eq:taylor2}
% \end{equation}

% With both these Taylor approximations (\ref{eq:taylor1}) and (\ref{eq:taylor2})
% in hand, we can write out an asymptotic expression for 
% the likelihood ratio.

% \begin{equation}
%     \begin{split}
%         R_L &\rightarrow -2\left(\ell(\hat{\theta}) - \ell_C(\hat{\theta}_C)\right)
%         \\
%         R_L &\rightarrow -2\left(
%             (\ell(\theta_\star) + \frac{1}{2}E_N^TE_N)
%             - (\ell_C(\theta_{\star,C}) + \frac{1}{2}E_{N,C}^T E_{N,C})
%         \right)
%     \end{split}
% \end{equation}
% Now, this next argument is bordering on circular, but is nevertheless an important last step
% to find the (asymptotic) distribution of $R_L$. Under $\mathcal{H}_1$
% we can state that the constrained and unconstrained log likelihoods $\ell(\theta_\star)$ and
% $\ell_C{\theta_{\star,C}}$ are related by the affine
% projection (\ref{eq:affinemap}) s.t. $\theta_\star = A^+b + \nullA\theta_{\star,C}$.
% Expanding $y[k] = \varphi^T\theta_\star$ with this projection gives gives equality in 
% $\ell(\theta_\star) = \ell_C{\theta_{\star,C}}$. The likelihood ratio then approaches:
% \begin{equation}
%     R_L \rightarrow E_N^TE_N - E_{N,C}^TE_{N,C}
% \end{equation}
% Since $E_N$ and $E_{N,C}$ are both standardly normally distributed with dimensions $2n$, $m$ respectively
% with $m<2n$, we have
% \begin{equation}
%     R_L \xrightarrow[N\rightarrow\infty]{\text{dist}} \chi^2(2n-m)
% \end{equation}
\printbibliography
\end{document}