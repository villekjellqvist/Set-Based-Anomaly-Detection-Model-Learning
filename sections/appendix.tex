% !TEX root = root.tex

\subsection{Proof of lemma~\ref{lemma:constproblem}}
\label{app:constproof}
\begin{proof}
    By assumption \ref{ass:NA_ass}, we have that $\Pinv A^+b = (A^+b)^T\Pinv = 0$.
    Using this fact and (\ref{eq:thetanull}) we expand from (\ref{eq:EFG})
    \begin{equation}
        \begin{gathered}
            \theta^T\left(\Pinv[k] + E[k]\right)\theta = \\
            = \sum\limits_{i=1}^k \bigg{[} \lambda^{k-i}\bigg{(}(A^+b)^T(\phi[i])^T\phi[i]A^+b
            + \\
            + 2(A^+b)^T(\phi[i])^T\phi[i]\nullA\tC + \tC^T\nullA^T(\phi[i])^T\phi[i]\nullA\tC \bigg{)} \bigg{]} + \\
            + \tC\nullA^T\Pinv\nullA\tC\lambda^{k+1}
        \end{gathered}
        \label{eq:expandedE}
    \end{equation}
    Likewise, we can do the same for 
    \begin{equation}
        \begin{gathered}
            \left(\theta_0^T\Pinv[k] + F[k]\right)\theta = \\
            = \sum\limits_{i=1}^k \lambda^{k-i}\left((y[k])^T\phi[k]A^+b + (y[k])^T\phi[k]\nullA\tC\right) + \\
            + \lambda^{k+1}\tC_0^T\nullA^T\Pinv[k]\nullA\tC
        \end{gathered}
        \label{eq:expandedF}
    \end{equation}
    To find the minimizing $\tC$ with respect to (\ref{eq:EFG}), we can discard
    all terms in (\ref{eq:expandedE}) and (\ref{eq:expandedF}) that do not depend on $\tC$.
    To get the remaining terms into their recursive form, replace all instances of
    $\lambda^{k+1}\Pinv$ with $P[k]$, $\lambda^{k-i}(\phi[i])^T\phi[i]$ with $E[k]$ and
    $\lambda^{k-i}(y[k])^T\phi[k]$ with $F[k]$. Finally to get (\ref{eq:Econst}) and (\ref{eq:Fconst}),
    gather all terms that are quadratic in $\tC$ into $\overline{E}[k]$ and all terms that are linear in
    $\tC$ into $\overline{F}[k]$.
\end{proof}

\subsection{Proof of lemma \ref{lem:tvsysmodel}}
\label{app:likelihoodproof}
Starting from (\ref{eq:tvsysmodel}), write out the probability of
$y[i]$ given the $n$ latest data points in $\phi[i]$
\begin{equation*}
    p(y[i]\big{\vert} \theta^*,  y_{[i-1:i-n]}, u_{[i-1:i-n]}) =
    \mathcal{N}(Y[i]\big{\vert}\phi[i]\theta^*, \frac{\gamma^2}{\lambda^{k-i}}).
\end{equation*}
Using Bayes rule, we can write the joint probability of $y[i]$ and $y[i-1$], much
in the same way as in \cite[Lemma 5.1]{ljung_system_1999}
\begin{equation*}
    \small
    \begin{aligned}
        &p(y[i],y[i-1]\big{\vert}\theta^*,y_{[i-2:i-n-1]}, u_{[i-1:i-n-1]}) = \\
        =& p(y[i]\big{\vert}y[i-1]=y[i-1], \theta^*, y_{[i-2:i-n-1]}, u_{[i-1:i-n-1]})\cdot \\
        &\cdot p(y[i-1]\big{\vert}\theta^*, y_{[i-2:i-n-1]}, u_{[i-1:i-n-1]}) = \\
        =& \mathcal{N}(y[i]\big{\vert}\phi[k]\theta^*, \frac{\gamma^2}{\lambda^{k-i}})\cdot
        \mathcal{N}(y[i-1]\big{\vert}\phi[i-1]\theta^*, \frac{\gamma^2}{\lambda^{k-i}}).
\end{aligned}
\notag
\end{equation*}
This can be done repeatedly, all the way until $k=1$, at which point we get
\begin{equation*}
    p(y_{[0:k]}\big{\vert}\theta^*, u_{[0:k]}) 
    = \prod\limits_{i=1}^k \mathcal{N}(y[i]\big{|}\phi[i]\theta^*, \frac{\gamma^2}{\lambda^{k-i}})
\end{equation*}
Next, take the log-likelihood with regards to $\theta^*$ to get (\ref{eq:loglikelihood}).
By inspection, it can be seen that (\ref{eq:loglikelihood}) attains its extremum 
with regards to $\theta^*$ at the same point as (\ref{eq:costfunction}), which 
concludes the proof.