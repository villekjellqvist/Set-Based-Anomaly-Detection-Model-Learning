% !TEX root = root.tex

\subsection{Proof of lemma~\ref{lemma:constproblem}}
\label{app:constproof}
By assumption \ref{ass:NA_ass}, we have that $\Pinv A^+b = (A^+b)^T\Pinv = 0$.
Using this fact and (\ref{eq:thetanull}) we expand from (\ref{eq:EFG})
\begin{equation}
    \begin{gathered}
        \theta^T\left(\Pinv[k] + E[k]\right)\theta = \\
        = \sum\limits_{i=1}^k \bigg{[} \lambda^{k-i}\bigg{(}(A^+b)^T(\phi[i])^T\phi[i]A^+b
        + \\
        + 2(A^+b)^T(\phi[i])^T\phi[i]\nullA\tC + \tC^T\nullA^T(\phi[i])^T\phi[i]\nullA\tC \bigg{)} \bigg{]} + \\
        + \tC\nullA^T\Pinv\nullA\tC\lambda^{k+1}
    \end{gathered}
    \label{eq:expandedE}
\end{equation}
Likewise, we can do the same for 
\begin{equation}
    \begin{gathered}
        \left(\theta_0^T\Pinv[k] + F[k]\right)\theta = \\
        = \sum\limits_{i=1}^k \lambda^{k-i}\left((y[k])^T\phi[k]A^+b + (y[k])^T\phi[k]\nullA\tC\right) + \\
        + \lambda^{k+1}\tC_0^T\nullA^T\Pinv[k]\nullA\tC
    \end{gathered}
    \label{eq:expandedF}
\end{equation}
To find the minimizing $\tC$ with respect to (\ref{eq:EFG}), we can discard
all terms in (\ref{eq:expandedE}) and (\ref{eq:expandedF}) that do not depend on $\tC$.
To get the remaining terms into their recursive form, replace all instances of
$\lambda^{k+1}\Pinv$ with $P[k]$, $\lambda^{k-i}(\phi[i])^T\phi[i]$ with $E[k]$ and
$\lambda^{k-i}(y[k])^T\phi[k]$ with $F[k]$. Finally to get (\ref{eq:Econst}) and (\ref{eq:Fconst}),
gather all terms that are quadratic in $\tC$ into $\overline{E}[k]$ and all terms that are linear in
$\tC$ into $\overline{F}[k]$.

\subsection{Proof of lemma \ref{lem:tvsysmodel}}
\label{app:likelihoodproof}
Starting from (\ref{eq:tvsysmodel}), write out the probability of
$y[i]$ given the $n$ latest data points in $\phi[i]$
\begin{equation*}
    p(y[i]\big{\vert} \theta^*,  y_{[i-1:i-n]}, u_{[i-1:i-n]}) =
    \mathcal{N}(Y[i]\big{\vert}\phi[i]\theta^*, \frac{\gamma^2}{\lambda^{k-i}}).
\end{equation*}
Using Bayes rule, we can write the joint probability of $y[i]$ and $y[i-1$], much
in the same way as in \cite[Lemma 5.1]{ljung_system_1999}
\begin{equation*}
    \small
    \begin{aligned}
        &p(y[i],y[i-1]\big{\vert}\theta^*,y_{[i-2:i-n-1]}, u_{[i-1:i-n-1]}) = \\
        =& p(y[i]\big{\vert}y[i-1]=y[i-1], \theta^*, y_{[i-2:i-n-1]}, u_{[i-1:i-n-1]})\cdot \\
        &\cdot p(y[i-1]\big{\vert}\theta^*, y_{[i-2:i-n-1]}, u_{[i-1:i-n-1]}) = \\
        =& \mathcal{N}(y[i]\big{\vert}\phi[k]\theta^*, \frac{\gamma^2}{\lambda^{k-i}})\cdot
        \mathcal{N}(y[i-1]\big{\vert}\phi[i-1]\theta^*, \frac{\gamma^2}{\lambda^{k-i}}).
\end{aligned}
\notag
\end{equation*}
This can be done repeatedly, all the way until $k=1$, at which point we get
\begin{equation*}
    p(y_{[0:k]}\big{\vert}\theta^*, u_{[0:k]}) 
    = \prod\limits_{i=1}^k \mathcal{N}(y[i]\big{|}\phi[i]\theta^*, \frac{\gamma^2}{\lambda^{k-i}})
\end{equation*}
Next, take the log-likelihood with regards to $\theta^*$ to get (\ref{eq:loglikelihood}).
By inspection, it can be seen that (\ref{eq:loglikelihood}) attains its extremum 
with regards to $\theta^*$ at the same point as (\ref{eq:costfunction}), which 
concludes the proof.