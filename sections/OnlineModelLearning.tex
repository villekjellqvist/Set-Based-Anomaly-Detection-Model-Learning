% !TEX root = root.tex

One frequently used method for real-time parameter estimation is recursive least squares
(RLS)\cite{astrom_adaptive_2008}. Here, we will use the same optimization problem but
combine it with elliptical constraints on the parameters, making use of prior knowledge
that the operator has. This will require the use of numerical optimization methods, instead 
of the closed form solution found in RLS. The cost function to be solved for the optimal
estimated parameters $\theta$ estimating the system (\ref{eq:sysmodel}) is
\begin{align}
        \arg\min\limits_{\theta} J[k](\theta) &\triangleq \notag
        \\ \triangleq \arg\min\limits_{\theta} &
            \sum\limits_{i=1}^k \left[
            \lambda^{k-i}(y[i]-\phi[i]\theta)^T(y[i]-\phi[i]\theta) + \right. \notag \\
            &+ \left. \lambda^{k+1}(\theta-\theta_0)^T\Pinv(\theta-\theta_0) \right]
    \label{eq:costfunction} \\
    \text{Subject to:}& \notag \\
    A\theta &= A\theta_0 = b
    \label{eq:eqconst} \\
    (\theta-\theta_0)^T&Q(\theta-\theta_0) \leq 1
    \label{eq:elliptconst}
\end{align}
Where $\lambda \in (0,1]$ is a forgetting factor and $\theta_0$
is the inital guess for the parameters
by the operator. $\Pinv \succeq 0$ is a weighting matrix representing the
operators confidence in the initial guess, while $Q \succeq 0$ defines an
ellipsoid around $\theta^0$ for the
inequality constraint (\ref{eq:elliptconst}). Since some linear combination
of the parameters in $\theta$ is already known and defined by the equality constraint
(\ref{eq:eqconst}), there is no need to weight those elements with $\Pinv$ and $Q$. We therefore
make the following assumption.

\VK{(Maybe $P_0$ should be used for the ellipsoidal inequality constraints
instead, as both $P_0$ and $Q$ represent the operators confidence in the intial guess
$\tC_0$? Less hyperparameters to tune.)}
\AT{That could be a good idea, but they play different roles. P is now a 'soft-constraint', whereas (12) is a hard constraint. If one formulates the Lagrangian w.r.t. (12), you get exactly something of the form of (6).}
\AT{It could be interesting to consider the special case of $\theta_0^C = 0$, since then the soft-constraint version of (12) becomes a weighted l2-regularization, which can be solved analytically.}

\begin{assumption}

\begin{color}{red}
    The matrices $Q, \Pinv$ are zero-weight for the known parameter combinations. Thus $\Pinv, Q \in \nullA$, where
    $\nullA = \ker A$ is a null space basis matrix for the equality constraint matrix $A$.
\end{color}
\VK{Is it ill-defined to claim that a matrix is part of a 
vector space? Since $Q, \Pinv$ are symmetric, their row and column space are the same, so maybe I
should say instead that their row and column vectors are members of $\nullA$?}
\label{ass:NA_ass}
\end{assumption}
It should be noted that 
$J[k](\theta)$ is the same cost function that is solved
by the RLS algorithm. \cite{islam_recursive_2019}

The prior knowledge that the operator has is represented by the (in)equality constraints
on $\theta$. These form the permissible model set for our parameter estimates, defined as
\begin{equation}
    \Mp = \left\{ \theta
    \vert
    (A\theta=b) \wedge (\theta-\theta_0)^TQ(\theta-\theta_0) \leq 1 \right\}.
\end{equation}
We have that
\begin{equation}
    \Pran = A^+A
\end{equation}
is the orthogonal projector onto the range of $A$ and
\begin{equation}
    \Port = I - \Pi_A = \nullA\nullA^T
\end{equation}
is the orthogonal projector onto the null space of $A$,
where $A^+ = A^T(AA^T)^{-1}$ denotes the pseudo inverse of $A$.
Let $\tC = \nullA^T\theta$ denote the parameter vector lying in the
equality unconstrained subspace of $\theta$-space. Since
$\Pran + \Port = I$ and using (\ref{eq:eqconst}) we have that
\begin{align}
    (\Pran + \Port)\theta &= A^+A\theta + \nullA\nullA^T\theta \notag \\
    \theta &= A^+b + \nullA\tC
    \label{eq:thetanull}
\end{align}

The same procedure can be done for the inital guess $\theta_0$.
Let $\tC_0 = \nullA^T\theta_0$ such that
\begin{equation}
    \theta_0 = A^+b + \nullA\tC_0.
\end{equation}
Then the inequality constraint (\ref{eq:elliptconst}) can be rewritten
\begin{equation}
    (\tC-\tC_0)^T\QC(\tC-\tC_0) \leq 1.
\end{equation}

\par
To get the real-time estimates of $\theta$, we need to minimize
(\ref{eq:costfunction}) at each sampling time $k$. First, note that the
cost function $J[k](\theta)$ can be rewritten as
\begin{equation}
    \begin{split}
        J[k](\theta) &= \sum\limits_{i=0}^k \left[
            \theta^T\left(P[k] + E[k]\right)\theta - \right.\\
        &- 2\left(\theta_0^TP[k] + F[k]\right)\theta +\\
        &+\left.\theta_0^TP[k]\theta_0 + G[k] \right]\\
        E[k] &= \sum\limits_{i=1}^{k} \lambda^{k-i}(\phi[i])^T\phi[i] \\
        F[k] &= \sum\limits_{i=1}^{k} \lambda^{k-i}(y[i])^T\phi[i] \\
        G[k] &= \sum\limits_{i=1}^{k} \lambda^{k-i}(y[i])^Ty[i] \\
        P[K] &= \lambda^{k+1}\Pinv
    \end{split}
    \label{eq:EFG}
\end{equation}
\VK{$\Pinv$ is only represented in its inverse form because that is how it's
presented in RLS. In the RLS case it makes more sense since the $P_k$ matrices directly
represent the covariance of the parameter estimate which one needs to take the inverse of at each
time point. That connection is not obvious here, and the covariance of the estimate is not really used
in any case.}
and that $G[k]$ is not dependent on $\theta$, and can thus be ignored in the optimization
problem without affecting the minimizing argument. The matrices
$E[k], F[k]$ and $\lambda^{k+1}\Pinv$ can be written recursively
\begin{align}
    E[k+1] &= \lambda E[k] + (\phi[k])^T\phi[k] \label{eq:Erec}\\
    F[k+1] &= \lambda F[k] + (y[k])^T\phi[k] \label{eq:Frec}\\
    P[k+1] &= \lambda P[k]; \, P[0] = \lambda\Pinv \label{eq:Prec}
\end{align}
We can now express the constrained optimization problem as the following lemma.

\begin{lemma}
Replacing $\theta, \theta_0$ with $\tC, \tC_0$, the optimization problem
(\ref{eq:costfunction}-\ref{eq:elliptconst}) can be reformulated as
\begin{align}
    \arg\min\limits_{\tC} J[k](\tC) &= \notag
        \\ = \arg\min\limits_{\tC} &
            \sum\limits_{i=1}^k \left[\tC^T\overline{E}[k]\tC - 2\overline{F}[k]\tC\right]
            \label{eq:Jconst}\\
    \text{Subject to:}& \notag \\
    (\tC-\tC_0)^T&\QC(\tC-\tC_0) \leq 1
    \label{eq:redelliptconst}
\end{align}
where
\begin{align}
        \overline{E}[k] &= \nullA^T\left(E[k] + \Pinv[k]\right)\nullA \label{eq:Econst}\\
        \overline{F}[k] &= F[k]\nullA + \tC_0^T\nullA^T\Pinv[k] - \notag \\
        &-(A^+b)^TE[k]\nullA \label{eq:Fconst}.
\end{align}
The proof of this lemma can be found in appendix \ref{app:constproof}.
\label{lemma:constproblem}
\end{lemma}
By use of lemma \ref{lemma:constproblem}, we can construct a method for constrained
parameter inference with recursive data collection. Let $T$ be the total amount of timesteps
that the identification problem is run for. The constrained problem is then solved by
algorithm \ref{alg:constrainedlearning}.
\begin{algorithm}
    \caption{Real Time Model Learning with Parameter Constraints}
    \label{alg:constrainedlearning}
    \Input{$\tC_0, \Pinv, A, b, Q$}
    $E[0], F[0] \gets 0$\;
    $k \gets 0$\;
    $\nullA \gets \ker A$\;
    \While{$k\leq T$}{
        Update $E[k], F[k], \Pinv[k]$ according to (\ref{eq:Erec}-\ref{eq:Prec})\;
        Update $\overline{E}[k], \overline{F}[k]$ according to (\ref{eq:Econst}, \ref{eq:Fconst})\;
        Solve the optimization problem (\ref{eq:Jconst}-\ref{eq:redelliptconst}) for $\tC$\;
        $\theta \gets A^+b + \nullA\tC$\;
        $k \gets k+1$\;
    }
\end{algorithm}