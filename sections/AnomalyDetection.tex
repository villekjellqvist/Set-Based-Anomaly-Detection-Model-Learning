% !TEX root = root.tex

To decide whether the system behaves nominally according to our prior knowledge and
defined model set $\Mp$, we will use two hypothesis testing.
Given our best constrained estimate $\theta$ obtained by algorithm \ref{alg:constrainedlearning},
define also $\tU$ as the unconstrained solution to (\ref{eq:costfunction}), disregarding
the constraints (\ref{eq:eqconst}-\ref{eq:elliptconst}).
Let $\hyp_0$ be the null hypothesis that the input-output data conforms to our predefined model set,
and $\hyp_1$ be the hypothesis that it does not.
Using the likelihood $\LH(\theta^* \big{\vert} y_{[k-1:k-n]}, u_{[k-1:k-n]})$
of any parameter vector $\theta^*$ given the data, we can define the
log-likelihood ratio test
\begin{equation}
    R_L = \ln\left(\frac{\sup\limits_{\theta \in \Mp}
    \LH(\theta \big{\vert} y_{[0:k]}, u_{[0:k]})}
    {\sup\limits_{\tU \in \mathbb{R}^m}
    \LH(\tU \big{\vert} y_{[0:k]}, u_{[0:k]})}\right)
\end{equation}
where a $R_L$ close to one means  $\hyp_0$ is likely to be true, and
inversely that a low $R_L$ close to zero means that $\hyp_1$ is likely true.
\VK{Since I will be using E[k] to calculate these likelihoods, I need to include the forgetting
factor $\lambda$ in the likelihoods. I am unsure how to do that notationally. While it might
not be as formally correct, it does make sense in the context of changing parameters.
Probably I will reformulate the problem as some kind of weighted Maximum likelihood.}

To actually find an expression for these likelihoods, we need to first find the probability
$p(Y_{[0:k]}\big{|}\theta^*, u_{[0:k]})$. Firstly, we need to consider the influence
of $\Pinv$ on the estimated $\theta$. It is clear from (\ref{eq:costfunction})
that for large $k$ and/or small $\Pinv$ that the initial guess $\theta_0$
has a negligible effect on the estimate. We therefore make the following
assumption henceforth.
\begin{assumption}
    \label{ass:smallP0}
    $\Pinv$ is small enough and the number of collected
    data points $k$ is large enough that
    \begin{equation}
        \begin{aligned}
            \arg\min\limits_{\theta} J[k](\theta) &\triangleq \notag
        \\ \triangleq \arg\min\limits_{\theta} &
            \sum\limits_{i=1}^k \left[
            \lambda^{k-i}(y[i]-\phi[i]\theta)^T(y[i]-\phi[i]\theta) \right]
        \end{aligned}
    \end{equation}
    is a good approximation of (\ref{eq:costfunction}). We therefore
    omit all terms containing $\Pinv$ from $\overline{E}[k],
    \overline{F}[k], E[k]$ and $F[k]$ in the sequel.
\end{assumption}

Because we have introduced a forgetting factor $\lambda$
in the parameter estimation, the model (\ref{eq:sysmodel})
is not actually the one we are estimating. The forgetting factor can be
interpreted as a time changing variance, which we state as the following lemma.
\begin{lemma}
    \label{lem:tvsysmodel}
    Under assumption \ref{ass:smallP0}, the cost function (\ref{eq:costfunction})
    is the maximum likelihood estimation of $\theta$ given the data generating model
    \begin{equation}
        y[i] = \phi[i]\theta_{\text{true}} + w_\lambda[i] \qquad w_\lambda[i]
        \sim \mathcal{N}\left(w_\lambda\big{\vert} 0, \frac{\gamma^2}{\lambda^{k-i}}\right)
        \label{eq:tvsysmodel}
    \end{equation}
    for all $i<k$, with the log-likelihood function
    \begin{equation}
        \begin{aligned}
            \ln&~\LH(\theta^*\big{|} y_{[0:k]}, u_{[0:k]}) = \sum\limits_{i=1}^{k}
            -\frac{k}{2}\ln\left(\frac{2\pi\gamma^2}{\lambda^{k-i}}\right) -
            \\-&
            \frac{\lambda^{k-i}}{2\gamma^2}(y[i]-\phi[i]\theta^*)^T(y[i]-\phi[i]\theta^*)
        \end{aligned}
        \label{eq:loglikelihood}
        \end{equation}
    Proof: See appendix \ref{app:likelihoodproof}
\end{lemma}
By expanding (\ref{eq:loglikelihood}), and substituting into $R_L$, we get
\begin{equation}
    \begin{aligned}
        R_L = \sum\limits_{i=1}^{k}\frac{\lambda^{k-i}}{\gamma^2} \big{[}
        y[i]^T\phi[t]\left(\theta - \theta_U\right)\\
        -\frac{1}{2}\left(\theta - \theta_U\right)^T
        \phi[i]^T\phi[i]\left(\theta - \theta_U\right)\big{]}
    \end{aligned}
\end{equation}

Thus, the likelihood ratio can be evaluated at each time update as such
\begin{equation}
    R_L = \left( F[k]\left(\theta - \theta_U\right) - 
    \frac{1}{2}\left(\theta - \theta_U\right)^TE[k]\left(\theta - \theta_U\right)  \right)
    \label{eq:likelihoodratio}
\end{equation}

and we have a combined parameter estimator and anomaly detector, as seen
seen in algorithm \ref{alg:anomalydetector}.
\begin{algorithm}
    \caption{Real Time Model Learning with Parameter Constraints and Anomaly Detection}
    \label{alg:anomalydetector}
    \Input{$\tC_0, \Pinv, A, b, Q, \gamma^2$}
    $E[0], F[0] \gets 0$\;
    $k \gets 0$\;
    $\nullA \gets \ker A$\;
    \While{$k\leq T$}{
        Update $E[k], F[k], \Pinv[k]$ according to (\ref{eq:Erec}-\ref{eq:Prec})\;
        Update $\overline{E}[k], \overline{F}[k]$ according to (\ref{eq:Econst}, \ref{eq:Fconst})\;
        Solve the optimization problem (\ref{eq:Jconst}-\ref{eq:redelliptconst}) for $\tC$\;
        Solve the unconstrained optimization problem (\ref{eq:costfunction}) for $\theta_U$\;
        $\theta \gets A^+b + \nullA\tC$\;
        Update $R_L$ according to (\ref{eq:likelihoodratio})\;
        $k \gets k+1$\;
    }
\end{algorithm}